{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n",
    "import tensorflow as tf\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import Doc2Vec\n",
    "import math\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pickle\n",
    "\n",
    "def tokenize(text: str, stopwords: set):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    return [i for i in text.split(\" \") if i not in stopwords]\n",
    "\n",
    "\n",
    "def join_name_description(name: str, desc: str):\n",
    "    name = name.lower()\n",
    "\n",
    "    if pd.isna(desc):\n",
    "        return name\n",
    "\n",
    "    desc = desc.lower()\n",
    "\n",
    "    if desc == \"no description yet\":\n",
    "        return name\n",
    "\n",
    "    return name + \" \" + desc\n",
    "\n",
    "\n",
    "def stem(words: list, stemmer: SnowballStemmer):\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def build_dataset(df):\n",
    "    df = df[df[\"item_condition_id\"].notna()]\n",
    "    df = df[df[\"price\"].notna()]\n",
    "    df = df[df[\"category_name\"].notna()]\n",
    "    df = df.drop(\"train_id\", axis=1)\n",
    "\n",
    "    categories = df[\"category_name\"].str.split(\"/\")\n",
    "    first_categories = categories.apply(\n",
    "        lambda x: x[0]).astype(\"category\").cat.codes\n",
    "    second_categories = categories.apply(\n",
    "        lambda x: x[1]).astype(\"category\").cat.codes\n",
    "\n",
    "    df[\"first_cat\"] = first_categories\n",
    "    df[\"second_cat\"] = second_categories\n",
    "\n",
    "    df = df.drop(\"category_name\", axis=1)\n",
    "    price = df[\"price\"]\n",
    "    df = df.drop(\"price\", axis=1)\n",
    "\n",
    "    X, X_test, Y, Y_test = train_test_split(\n",
    "        df, price, test_size=0.3, stratify=df[\"first_cat\"], shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X, Y, test_size=0.2, stratify=X[\"first_cat\"], shuffle=True)\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "\n",
    "def one_hot_encode(dataset):\n",
    "    res = pd.DataFrame(np_utils.to_categorical(\n",
    "        dataset[\"item_condition_id\"] - 1, 5))\n",
    "    res = res.set_axis([\"item_condition_\" + str(x)\n",
    "                       for x in range(5)], axis=1, copy=True)\n",
    "\n",
    "    firstcat_onehot = np_utils.to_categorical(dataset[\"first_cat\"])\n",
    "    secondcat_onehot = np_utils.to_categorical(dataset[\"second_cat\"])\n",
    "\n",
    "    tmp = pd.DataFrame(firstcat_onehot)\n",
    "    tmp = tmp.set_axis([\"first_cat_\" + str(x)\n",
    "                       for x in range(10)], axis=1, copy=True)\n",
    "    res = pd.concat([res, tmp], axis=1)\n",
    "\n",
    "    num_sec_cat = len(dataset[\"second_cat\"].unique())\n",
    "    tmp = pd.DataFrame(secondcat_onehot)\n",
    "    tmp = tmp.set_axis([\"second_cat_\" + str(x)\n",
    "                       for x in range(num_sec_cat)], axis=1, copy=True)\n",
    "    res = pd.concat([res, tmp], axis=1)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_stemmed_text(dataset):\n",
    "    names = dataset[\"name\"]\n",
    "    descriptions = dataset[\"item_description\"]\n",
    "\n",
    "    combined = names.combine(descriptions, join_name_description)\n",
    "\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokenized = combined.apply(lambda x: tokenize(x, stop))\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return tokenized.apply(lambda x: stem(x, stemmer))\n",
    "\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def train_doc2vec(text_data):   \n",
    "  print('start training doc2vec')\n",
    "  data_for_training = list(tagged_document(text_data))\n",
    "  model = gensim.models.doc2vec.Doc2Vec(\n",
    "      vector_size=500, min_count=2, epochs=30)\n",
    "  model.build_vocab(data_for_training)\n",
    "  model.train(data_for_training, total_examples=model.corpus_count,\n",
    "              epochs=model.epochs)\n",
    "  print('end training doc2vec')\n",
    "  # print(model.infer_vector(\n",
    "  #     ['violent', 'means', 'to', 'destroy', 'the', 'organization']))\n",
    "  return model\n",
    "\n",
    "def preprocess(train, val, test):\n",
    "    # train set\n",
    "    X_train = one_hot_encode(train)\n",
    "    X_train[\"shipping\"] = train[\"shipping\"]\n",
    "    X_train[\"shipping\"] = X_train[\"shipping\"].fillna(0.5)\n",
    "    stemmed_train = get_stemmed_text(train)\n",
    "\n",
    "    # validation set\n",
    "    X_val = one_hot_encode(val)\n",
    "    X_val[\"shipping\"] = val[\"shipping\"]\n",
    "    X_val[\"shipping\"] = X_val[\"shipping\"].fillna(0.5)\n",
    "    stemmed_val = get_stemmed_text(val)\n",
    "\n",
    "    # test set\n",
    "    X_test = one_hot_encode(test)\n",
    "    X_test[\"shipping\"] = test[\"shipping\"]\n",
    "    X_test[\"shipping\"] = X_test[\"shipping\"].fillna(0.5)\n",
    "    stemmed_test = get_stemmed_text(test)\n",
    "\n",
    "    # embedding-------------------------------------------------\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "\n",
    "    # tr_text_df = pd.DataFrame.sparse.from_spmatrix(tr_text)\n",
    "\n",
    "    # n_features = tr_text_df.shape[1]\n",
    "    # features_names = [\"text_\" + str(x) for x in range(n_features)]\n",
    "\n",
    "    # tr_text_df = tr_text_df.set_axis(features_names, axis=1, copy=True)\n",
    "\n",
    "    # val_text_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    #     val_text).set_axis(features_names, axis=1, copy=True)\n",
    "    # te_text_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    #     te_text).set_axis(features_names, axis=1, copy=True)\n",
    "\n",
    "    # X_train = pd.concat([X_train, tr_text], axis=1)\n",
    "    # X_val = pd.concat([X_val, val_text], axis=1)\n",
    "    # X_test = pd.concat([X_test, te_text], axis=1)\n",
    "\n",
    "    return X_train, X_val, X_test, stemmed_train, stemmed_test, stemmed_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = build_dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_train)\n",
    "Y_train = Y_train.apply(lambda x: np.log(x+0.1))\n",
    "Y_test = Y_test.apply(lambda x: np.log(x+0.1))\n",
    "Y_val = Y_val.apply(lambda x: np.log(x+0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, stemmed_train, stemmed_test, stemmed_val = preprocess(\n",
    "    X_train, X_val, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_train = stemmed_train.to_numpy()\n",
    "stemmed_test = stemmed_test.to_numpy()\n",
    "stemmed_val = stemmed_val.to_numpy()\n",
    "\n",
    "stemmed_train = [x.split(' ') for x in stemmed_train]\n",
    "stemmed_test = [x.split(' ') for x in stemmed_test]\n",
    "stemmed_val = [x.split(' ') for x in stemmed_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training doc2vec\n",
      "end training doc2vec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m doc2vecmodel \u001b[39m=\u001b[39m train_doc2vec(stemmed_train)\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./ProcessedData/doc2vec.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(f, doc2vecmodel)\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "doc2vecmodel = train_doc2vec(stemmed_train)\n",
    "with open('./ProcessedData/doc2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(f, doc2vecmodel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text = [doc2vecmodel.infer_vector(x) for x in stemmed_train]\n",
    "val_text = [doc2vecmodel.infer_vector(x) for x in stemmed_val]\n",
    "te_text = [doc2vecmodel.infer_vector(x) for x in stemmed_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ProcessedData/doc2vec.pkl', 'wb') as f:\n",
    "  pickle.dump(doc2vecmodel, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train, tr_text], axis=1)\n",
    "X_val = np.concatenate([X_val, val_text], axis=1)\n",
    "X_test = np.concatenate([X_test, te_text], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./ProcessedData/x_train_doc2vec.pkl','wb') as f:\n",
    "  pickle.dump(X_train, f)\n",
    "with open('./ProcessedData/x_val_doc2vec.pkl', 'wb') as f:\n",
    "  pickle.dump(X_val, f)\n",
    "with open('./ProcessedData/x_test_doc2vec.pkl', 'wb') as f:\n",
    "  pickle.dump(X_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('./ProcessedData/x_train_doc2vec.pkl', 'rb') as f:\n",
    "#   X_train = pickle.load(f)\n",
    "# with open('./ProcessedData/x_val_doc2vec.pkl', 'rb') as f:\n",
    "#   X_val = pickle.load(f)\n",
    "with open('./ProcessedData/x_test_doc2vec.pkl', 'rb') as f:\n",
    "  X_test = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(826676, 629)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(512)\n",
    "\n",
    "train_dataset.save('./ProcessedData/tf_train_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_val, Y_val)).batch(512)\n",
    "\n",
    "validation_dataset.save('./ProcessedData/tf_validation_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, Y_test)).batch(512)\n",
    "\n",
    "test_dataset.save('./ProcessedData/tf_test_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.load(\n",
    "    './ProcessedData/tf_train_dataset_doc2vec')\n",
    "validation_dataset = tf.data.Dataset.load(\n",
    "    './ProcessedData/tf_validation_dataset_doc2vec')\n",
    "test_dataset = tf.data.Dataset.load(\n",
    "    './ProcessedData/tf_test_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, Y_train)).batch(512)\n",
    "\n",
    "train_dataset.save('./ProcessedData/tf_train_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, Y_train)).batch(512)\n",
    "\n",
    "tf.data.experimental.save(\n",
    "    train_dataset, './ProcessedData/tf_train_dataset_doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 629)]             0         \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (None, 629, 10)           200       \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 629, 64)           14592     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 40256)             0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 256)               10305792  \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,363,849\n",
      "Trainable params: 10,363,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cherrypicked_seed = 42\n",
    "\n",
    "input = Input(shape=(629,))\n",
    "emb1 = tf.keras.layers.Embedding(20, 10, input_length=1)(input)  # EMBEDDING 1\n",
    "gru1 = tf.keras.layers.GRU(64, return_sequences=True)(emb1)\n",
    "flat1 = tf.keras.layers.Flatten()(gru1)\n",
    "\n",
    "\n",
    "hidden3 = Dense(256, activation=LeakyReLU(alpha=0.3))(flat1)\n",
    "\n",
    "# gru2 = tf.keras.layers.GRU(32, return_sequences=True)(flat2)\n",
    "# flat2 = tf.keras.layersFlatten()(gru2)\n",
    "hidden4 = Dense(128, activation=LeakyReLU(alpha=0.3))(hidden3)\n",
    "dropout4 = Dropout(0.1, seed=cherrypicked_seed)(hidden4)\n",
    "\n",
    "hidden5 = Dense(64, activation=LeakyReLU(alpha=0.3))(dropout4)\n",
    "dropout5 = Dropout(0.1, seed=cherrypicked_seed)(hidden5)\n",
    "\n",
    "hidden6 = Dense(32, activation=LeakyReLU(alpha=0.3))(dropout5)\n",
    "dropout6 = Dropout(0.1, seed=cherrypicked_seed)(hidden6)\n",
    "\n",
    "output = Dense(1, activation=\"linear\")(dropout6)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(clipnorm=1)\n",
    "model.compile(optimizer=opt, loss=\"mean_squared_error\", metrics='mse')\n",
    "model.summary()\n",
    "\n",
    "fBestModel = 'best_model_word2vec.h5'\n",
    "early_stop = EarlyStopping(monitor='val_mse', mode=\"min\", patience=15,\n",
    "                           min_delta=0.01, verbose=1, restore_best_weights=True)\n",
    "best_model = ModelCheckpoint(\n",
    "    fBestModel, verbose=1, save_best_only=True, monitor='val_mse', mode=\"min\")\n",
    "\n",
    "# logdir = os.path.join(\"logs\", \"dropout_model\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1614/1615 [============================>.] - ETA: 0s - loss: 0.7260 - mse: 0.7260\n",
      "Epoch 1: val_mse improved from inf to 0.63289, saving model to best_model_word2vec.h5\n",
      "1615/1615 [==============================] - 98s 60ms/step - loss: 0.7260 - mse: 0.7260 - val_loss: 0.6329 - val_mse: 0.6329\n",
      "Epoch 2/1000\n",
      "1614/1615 [============================>.] - ETA: 0s - loss: 0.6684 - mse: 0.6684\n",
      "Epoch 2: val_mse improved from 0.63289 to 0.63239, saving model to best_model_word2vec.h5\n",
      "1615/1615 [==============================] - 97s 60ms/step - loss: 0.6685 - mse: 0.6685 - val_loss: 0.6324 - val_mse: 0.6324\n",
      "Epoch 3/1000\n",
      "1614/1615 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.6520\n",
      "Epoch 3: val_mse did not improve from 0.63239\n",
      "1615/1615 [==============================] - 112s 70ms/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.6329 - val_mse: 0.6329\n",
      "Epoch 4/1000\n",
      "1614/1615 [============================>.] - ETA: 0s - loss: 0.6449 - mse: 0.6449\n",
      "Epoch 4: val_mse improved from 0.63239 to 0.63206, saving model to best_model_word2vec.h5\n",
      "1615/1615 [==============================] - 113s 70ms/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.6321 - val_mse: 0.6321\n",
      "Epoch 5/1000\n",
      "1614/1615 [============================>.] - ETA: 0s - loss: 0.6414 - mse: 0.6414\n",
      "Epoch 5: val_mse improved from 0.63206 to 0.63193, saving model to best_model_word2vec.h5\n",
      "1615/1615 [==============================] - 111s 69ms/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 6/1000\n",
      "1615/1615 [==============================] - ETA: 0s - loss: 0.6452 - mse: 0.6452\n",
      "Epoch 6: val_mse did not improve from 0.63193\n",
      "1615/1615 [==============================] - 112s 69ms/step - loss: 0.6452 - mse: 0.6452 - val_loss: 0.6328 - val_mse: 0.6328\n",
      "Epoch 7/1000\n",
      "1615/1615 [==============================] - ETA: 0s - loss: 0.6379 - mse: 0.6379\n",
      "Epoch 7: val_mse did not improve from 0.63193\n",
      "1615/1615 [==============================] - 108s 67ms/step - loss: 0.6379 - mse: 0.6379 - val_loss: 0.6322 - val_mse: 0.6322\n",
      "Epoch 8/1000\n",
      " 697/1615 [===========>..................] - ETA: 51s - loss: 0.6325 - mse: 0.6325"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[0;32m      2\u001b[0m n_workers \u001b[39m=\u001b[39m \u001b[39m12\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      5\u001b[0m     train_dataset,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      8\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m      9\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[best_model, early_stop],\n\u001b[0;32m     10\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  \n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "n_workers = 12\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[best_model, early_stop],\n",
    "    verbose=1,  \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e850eec08e9669c52e782ee28b71a07544c9e38f5af88e96f29f06e82864309f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
